# Copyright (C) 2000 Peter McGavin.
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  
#
# See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.

		.section	".rodata"
		.align 2
.LC6:
		.long 0x41000000	# 8.0
		.align 3
.LC7:
		.long 0x43300000
		.long 0x80000000
		.align 2
.LC8:
		.long 0x47800000
		.align 2
.reciprocals:
		.long 0x55555555	# 1/3
		.long 0x40000000	# 1/4
		.long 0x33333333	# 1/5
		.long 0x2aaaaaab	# 1/6
		.long 0x24924925	# 1/7
		.long 0x20000000	# 1/8
		.long 0x1c71c71c	# 1/9
		.long 0x1999999a	# 1/10
		.long 0x1745d174	# 1/11
		.long 0x15555555	# 1/12
		.long 0x13b13b14	# 1/13
		.long 0x12492492	# 1/14
		.long 0x11111111	# 1/15
.LC6a:
		.long 0x41800000	# 16.0

########################################################################
		.section	".text"

		.global	D_DrawSpans8
		.type	D_DrawSpans8,@function
		.global	_D_DrawSpans8
		.type	_D_DrawSpans8,@function

		.extern	d_sdivzstepu
		.extern	d_tdivzstepu
		.extern	d_sdivzstepv
		.extern	d_tdivzstepv
		.extern	d_zistepu
		.extern	d_zistepv
		.extern	d_tdivzorigin
		.extern	d_sdivzorigin
		.extern	d_viewbuffer
		.extern	cacheblock
		.extern	cachewidth
		.extern	d_ziorigin
		.extern	sadjust
		.extern	tadjust
		.extern	screenwidth
		.extern	bbextents
		.extern	bbextentt

		.align	3

# register usage (incomplete):
# r0				f0
# r1	sp			f1
# r2				f2
# r3	pspan			f3
# r4				f4
# r5				f5
# r6				f6
# r7	tmp			f7
# r8	tmp			f8
# r9	tmp			f9
# r10				f10
# r11	tmp			f11
# r12				f12	tmp
# r13				f13	tmp
# r14				f14
# r15				f15
# r16				f16
# r17	.reciprocals		f17
# r18	cachewidth		f18
# r19				f19
# r20	0x4330			f20
# r21	sadjust			f21
# r22	bbextents		f22
# r23	tadjust			f23
# r24	bbextentt		f24
# r25	pspan			f25
# r26	sstep			f26
# r27	tstep			f27	<unused>
# r28	count			f28	d_tdivzstepu
# r29	spancount		f29	d_sdivzstepu
# r30	pbase			f30	d_zistepu
# r31	pdest			f31	tmp

D_DrawSpans8:
_D_DrawSpans8:

# This code mostly generated by GCC 2.95.2

		stwu	r1,-128(r1)
		mflr	r0
		stfd	f27,88(r1)
		stfd	f28,96(r1)
		stfd	f29,104(r1)
		stfd	f30,112(r1)
		stfd	f31,120(r1)
		stw	r14,16(r1)
		stw	r15,20(r1)
		stw	r16,24(r1)
		stw	r17,28(r1)
		stw	r18,32(r1)
		stw	r19,36(r1)
		stw	r20,40(r1)
		stw	r21,44(r1)
		stw	r22,48(r1)
		stw	r23,52(r1)
		stw	r24,56(r1)
		stw	r25,60(r1)
		stw	r26,64(r1)
		stw	r27,68(r1)
		stw	r28,72(r1)
		stw	r29,76(r1)
		stw	r30,80(r1)
		stw	r31,84(r1)
		stw	r0,132(r1)

		lis	r11,bbextentt@ha
		lwz	r24,bbextentt@l(r11)

		lis	r9,tadjust@ha
		lwz	r23,tadjust@l(r9)

		lis	r11,bbextents@ha
		lwz	r22,bbextents@l(r11)

		lis	r9,sadjust@ha
		lwz	r21,sadjust@l(r9)

		lis	r20,0x4330

		lis	r10,d_zistepu@ha
		lfs	f30,d_zistepu@l(r10)

		lis	r9,d_sdivzstepu@ha
		lfs	f29,d_sdivzstepu@l(r9)

		lis	r11,d_tdivzstepu@ha
		lfs	f28,d_tdivzstepu@l(r11)

		lis	r9,cachewidth@ha
		lwz	r18,cachewidth@l(r9)

		lis	r9,.reciprocals-16@ha
		la	r17,.reciprocals-16@l(r9)

		lis	r7,.LC6@ha
		lfs	f0,.LC6@l(r7)

		lis	r9,cacheblock@ha
		lwz	r30,cacheblock@l(r9)

		lis	r8,.LC7@ha
		lfd	f3,.LC7@l(r8)

		lis	r7,d_tdivzorigin@ha
		mtlr	r7

		mr	r25,r3			# pspan

		fmuls	f31,f29,f0		# d_sdivzstepu
		li	r26,0			# sstep = 0
		li	r27,0			# tstep = 0

		lis	r19,d_sdivzorigin@ha
		fmuls	f1,f28,f0		# d_tdivzstepu
		lis	r14,d_viewbuffer@ha
		fmuls	f2,f30,f0		# d_zistepu
		lis	r16,0x4330
		lis	r15,d_ziorigin@ha

# outer loop starts here

.L140:
		lwz	r29,0(r25)
		lis	r9,d_zistepv@ha
		lwz	r3,4(r25)
		lis	r8,.LC8@ha
		xoris	r0,r29,0x8000
		lfs	f8,d_zistepv@l(r9)
		la	r8,.LC8@l(r8)
		stw	r0,12(r1)
		xoris	r9,r3,0x8000
		lis	r6,d_sdivzstepv@ha
		stw	r16,8(r1)
		lis	r4,d_tdivzstepv@ha
		lfd	r12,8(r1)
		lis	r5,screenwidth@ha
		stw	r9,12(r1)
		stw	r16,8(r1)
		lfd	f13,8(r1)
		fsub	f12,f12,f3
		lfs	f11,d_ziorigin@l(r15)
		fsub	f13,f13,f3
		lfs	f4,0(r8)
		frsp	f12,f12
		lfs	f10,d_sdivzorigin@l(r19)
		lfs	f0,d_sdivzstepv@l(r6)
		frsp	f13,f13
		mflr	r7
		lfs	f6,d_tdivzstepv@l(r4)
		fmadds	f8,f13,f8,f11
		lwz	r9,screenwidth@l(r5)
		fmadds	f0,f13,f0,f10
		lfs	f11,d_tdivzorigin@l(r7)
		mullw	r9,r9,r3
		fmadds	f5,f12,f30,f8		# d_zistepu
		lwz	r0,d_viewbuffer@l(r14)
		fmadds	f7,f12,f29,f0		# d_sdivzstepu
		fmadds	f13,f13,f6,f11
		add	r0,r0,r9
		lwz	r28,8(r25)
		fdivs	f4,f4,f5
		add	r31,r0,r29
		fmuls	f0,f7,f4
		fmadds	f8,f12,f28,f13		# d_tdivzstepu
		fctiwz	f9,f0
		stfd	f9,8(r1)
		lwz	r8,12(r1)
		add	r5,r8,r21		# +sadjust
		cmpw	cr0,r5,r22		# ,bbextents
		ble	.L109
		mr	r5,r22			# =bbextents
		b	.L110
.L109:
		nor	r0,r5,r5
		srawi	r0,r0,31
		and	r5,r5,r0
.L110:
		fmuls	f0,f8,f4
		fctiwz	f13,f0
		stfd	f13,8(r1)
		lwz	r10,12(r1)
		add	r6,r10,r23		# +tadjust
		cmpw	cr0,r6,r24		# ,bbextentt
		ble	.L112
		mr	r6,r24
		b	.L113
.L112:
		nor	r0,r6,r6
		srawi	r0,r0,31
		and	r6,r6,r0
.L113:
		cmpwi	cr0,r28,8
		lis	r8,.LC8@ha
		la	r8,.LC8@l(r8)
		lfs	f9,0(r8)

#  if (count > 8)
#    zi += zi8stepu;
#  else {
#    spancountminus1 = (float)(count - 1);
#    zi += d_zistepu * spancountminus1;
#  }
#  z = (float)0x10000 / zi;

		ble-	cr0,.xx1
		fadds	f5,f5,f2	# zi += zi8stepu
		b	.xx2
.xx1:
		addi	r0,r28,-1	# r0 = count - 1
		xoris	r0,r0,0x8000
		stw	r0,12(r1)
		stw	r20,8(r1)	# 0x4330
		lfd	f0,8(r1)
		fsubs	f0,f0,f3	# f0 = (float)(count - 1)
		fmadds	f5,f30,f0,f5	# zi += d_zistepu * spancountminus1
.xx2:
		fdivs	f4,f9,f5	# z = 0x10000 / zi   (18 cycles)

# CPU-intensive loop starts here
# This loop is executed once for every 8 pixels in a span

# do {

.L139:

#  if (count >= 8)
#    spancount = 8;
#  else
#    spancount = count;
#  count -= spancount;
#  if (count) {

		li	r29,8		# spancount = 8
		cmpwi	cr0,r28,8
		bge+	.L1
		mr	r29,r28		# spancount = count
.L1:
		subf.	r28,r29,r28	# count -= spancount
		beq-	.L120

#    sdivz += sdivz8stepu;
#    tdivz += tdivz8stepu;

		fadds	f7,f7,f31	# sdivz += sdivz8stepu
		fadds	f8,f8,f1	# tdivz += tdivz8stepu

#    snext = (int)(sdivz * z) + sadjust;
#    tnext = (int)(tdivz * z) + tadjust;
#    if (snext > bbextents)
#      snext = bbextents;
#    else if (snext < 8)
#      snext = 8;
#    if (tnext > bbextentt)
#      tnext = bbextentt;
#    else if (tnext < 8)
#      tnext = 8;

		fmuls	f0,f7,f4	# f0 = sdivz * z
		fmuls	f6,f8,f4	# f6 = tdivz * z
		fctiwz	f13,f0		# f13 = (int)(sdivz * z)
		fctiwz	f10,f6		# f10 = (int)(sdivs * z)
		stfd	f13,8(r1)
		lwz	r9,12(r1)	# r9 = (int)(tdivz * z)
		stfd	f10,8(r1)
		add	r12,r9,r21	# snext = r9 + sadjust
		lwz	r8,12(r1)	# r8 = (int)(sdivz * z)
		cmpwi	cr7,r28,8
		cmpw	cr0,r12,r22	# snext > bbextents?
		add	r4,r8,r23	# tnext = r8 + tadjust
		cmpwi	cr5,r12,8	# snext < 8?

#    if (count > 8)
#      zi += zi8stepu;
#    else {
#      spancountminus1 = (float)(count - 1);
#      zi += d_zistepu * spancountminus1;
#    }
#    z = (float)0x10000 / zi;

		ble-	cr7,.xx3
		fadds	f5,f5,f2	# zi += zi8stepu
		b	.xx4
.xx3:
		addi	r0,r28,-1	# r0 = count - 1
		xoris	r0,r0,0x8000
		stw	r0,12(r1)
		stw	r20,8(r1)	# 0x4330
		lfd	f0,8(r1)
		fsubs	f0,f0,f3	# f0 = (float)(count - 1)
		fmadds	f5,f30,f0,f5	# zi += d_zistepu * spancountminus1
.xx4:
		cmpw	cr1,r4,r24	# tnext > bbextentt?
		cmpwi	cr6,r4,8	# tnext < 8?
		fdivs	f4,f9,f5	# z = 0x10000 / zi   (18 cycles)

		ble+	cr0,.L121
		mr	r12,r22		# snext = bbextents
		b	.L122
.L121:
		bge+	cr5,.L122
		li	r12,8		# snext = 8
.L122:
		ble+	cr1,.L124
		mr	r4,r24		# tnext = bbextentt
		b	.L125
.L124:
		bge+	cr6,.L125
		li	r4,8		# tnext = 8
.L125:

#    sstep = (snext - s) >> 3;
#    tstep = (tnext - t) >> 3;

		subf	r0,r5,r12	# r0 = snext - s
		subf	r9,r6,r4	# r9 = tnext - t
		srawi	r26,r0,3	# sstep = (snext - s) >> 3
		srawi	r27,r9,3	# tstep = (tnext - t) >> 3
		b	.L127

.L120:

# } else {
#    sdivz += d_sdivzstepu * spancountminus1;
#    tdivz += d_tdivzstepu * spancountminus1;

		fmadds	f7,f29,f0,f7	# sdivz += d_sdivzstepu * spancountminus1;
		fmadds	f8,f28,f0,f8	# tdivz += d_tdivzstepu * spancountminus1;

#    snext = (int)(sdivz * z) + sadjust;
#    if (snext > bbextents)
#      snext = bbextents;
#    else if (snext < 8)
#      snext = 8;
#    tnext = (int)(tdivz * z) + tadjust;
#    if (tnext > bbextentt)
#      tnext = bbextentt;
#    else if (tnext < 8)
#      tnext = 8;

		fmuls	f0,f7,f4
		fmuls	f6,f8,f4
		fctiwz	f13,f0
		fctiwz	f10,f6
		stfd	f13,8(r1)
		lwz	r11,12(r1)
		stfd	f10,8(r1)
		lwz	r9,12(r1)
		add	r12,r11,r21	# +sadjust
		add	r4,r9,r23	# +tadjust
		cmpw	cr0,r12,r22	# ,bbextents
		cmpw	cr1,r4,r24	# ,bbextentt
		ble+	.L128
		mr	r12,r10
		b	.L129
.L128:
		cmpwi	cr0,r12,8
		bge+	.L129
		li	r12,8
.L129:
		ble+	cr1,.L131
		mr	r4,r24		# =bbextentt
		b	.L132
.L131:
		cmpwi	cr0,r4,8
		bge+	.L132
		li	r4,8
.L132:

#    if (spancount > 1) {
#      sstep = (snext - s) / (spancount - 1);
#      tstep = (tnext - t) / (spancount - 1);
#    }

		cmpwi	cr0,r29,2
		cmpwi	cr1,r29,3	# reciprocal doesn't work for 2
		blt-	.L127
		subf	r26,r5,r12
		subf	r27,r6,r4
		beq-	.L127		# no need to divide by 1
		bne+	cr1,.L4
		srawi	r26,r26,1	# sstep = (snext - s) >> 1
		srawi	r27,r27,1	# tstep = (tnext - t) >> 1
		b	.L127
.L4:
		slwi	r9,r29,2
		lwzx	r0,r9,r17	# get reciprocal from table
		mulhw	r26,r0,r26	# faster to multiply by reciprocal
		mulhw	r27,r0,r27
.L127:

# Following is the inner texture-mapping main loop of D_DrawSpans8()
# where the CPU probably spends most of its time --- except perhaps for
# the earlier fmuls and fdiv near .L139 and .L120.  Unfortunately I
# think I haven't saved a single cycle here over the code generated by
# GCC, but I made it a lot shorter.  It seems to be limited by the speed
# of memory.  The multiply by cachewidth for every pixel is a bottleneck
# too.

# If I've calculated it right, my loop takes 4 cycles per pixel, plus
# setup time --- not bad considering each mullw takes 4 cycles, each
# lbzx takes 2 and each stb takes 3.

# Since spancount is always between 1 and 8, perhaps it could be
# rewritten as a number of cases where several bytes are written to
# memory at once, with sth and stw, preferably aligned.

#  do {
#    *pdest++ = *(pbase + (s >> 16) + (t >> 16) * cachewidth);
#    s += sstep;
#    t += tstep;
#  } while (--spancount > 0);

		srawi	r11,r6,16	# SCIU1 1     r11 = t >> 16
		srawi	r9,r5,16	# SCIU2 1     r9 = s >> 16
		mullw	r11,r11,r18	# MCIU  2-5   r11 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 2     --spancount
		add	r6,r6,r27	# SCIU2 2     t += tstep
		add	r5,r5,r26	# SCIU1 3     s += sstep
		add	r9,r30,r9	# SCIU2 3     r9 = pbase + (s >> 16)
		srawi	r7,r6,16	# SCIU1 4     r7 = t >> 16
		srawi	r8,r5,16	# SCIU2 4     r8 = s >> 16
		lbzx	r0,r9,r11	# LSU   6-7   r0 = *(pbase + ...)
		beq-	.endloop	# BPU   6
.loop:
		mullw	r7,r7,r18	# MCIU  7-10  r7 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 7     --spancount
		add	r6,r6,r27	# SCIU2 7     t += tstep
		add	r5,r5,r26	# SCIU1 8     s += sstep
		add	r8,r30,r8	# SCIU2 8     r8 = pbase + (s >> 16)

		stb	r0,0(r31)	# LSU   8-10   *pdest = ...

		srawi	r11,r6,16	# SCIU1 9     r11 = t >> 16
		srawi	r9,r5,16	# SCIU2 9     r9 = s >> 16

		addi	r31,r31,1	# SCIU1 10    pdest++

		lbzx	r0,r8,r7	# LSU   10-11 r0 = *(pbase + ...)
		beq-	.endloop	# BPU   10

		mullw	r11,r11,r18	# MCIU  11-14 r11 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 11    --spancount
		add	r6,r6,r27	# SCIU2 11    t += tstep
		add	r5,r5,r26	# SCIU1 12    s += sstep
		add	r9,r30,r9	# SCIU2 12    r9 = pbase + (s >> 16)

		stb	r0,0(r31)	# LSU   12-14 *pdest = ...

		srawi	r7,r6,16	# SCIU1 13    r7 = t >> 16
		srawi	r8,r5,16	# SCIU2 13    r8 = s >> 16

		addi	r31,r31,1	# SCIU1 14    pdest++

		lbzx	r0,r9,r11	# LSU   14-15 r0 = *(pbase + ...)
		bne+	.loop		# BPU   14

.endloop:
		cmpwi	cr7,r28,0	# count > 0?
		stb	r0,0(31)	# *pdest = ...
		mr	r5,r12		# s = snext
		mr	r6,r4		# t = tnext
		addi	r31,r31,1	# pdest++
		bgt+	cr7,.L139
		lwz	r25,12(r25)	# pspan->pnext
		cmpwi	cr0,r25,0
		bne+	.L140
		lwz	r0,132(r1)
		mtlr	r0
		lwz	r14,16(r1)
		lwz	r15,20(r1)
		lwz	r16,24(r1)
		lwz	r17,28(r1)
		lwz	r18,32(r1)
		lwz	r19,36(r1)
		lwz	r20,40(r1)
		lwz	r21,44(r1)
		lwz	r22,48(r1)
		lwz	r23,52(r1)
		lwz	r24,56(r1)
		lwz	r25,60(r1)
		lwz	r26,64(r1)
		lwz	r27,68(r1)
		lwz	r28,72(r1)
		lwz	r29,76(r1)
		lwz	r30,80(r1)
		lwz	r31,84(r1)
		lfd	r27,120(r1)
		lfd	r28,112(r1)
		lfd	r29,104(r1)
		lfd	r30,96(r1)
		lfd	r31,88(r1)
		la	r1,128(r1)
		blr
.Lfe4:
		.size	D_DrawSpans8,.Lfe4-D_DrawSpans8

########################################################################
		.section	".text"

		.global	D_DrawSpans16
		.type	D_DrawSpans16,@function
		.global	_D_DrawSpans16
		.type	_D_DrawSpans16,@function

		.align	3

# register usage (incomplete):
# r0				f0
# r1	sp			f1
# r2				f2
# r3	pspan			f3
# r4				f4
# r5				f5
# r6				f6
# r7	tmp			f7
# r8	tmp			f8
# r9	tmp			f9
# r10				f10
# r11	tmp			f11
# r12				f12	tmp
# r13				f13	tmp
# r14				f14
# r15				f15
# r16				f16
# r17	.reciprocals		f17
# r18	cachewidth		f18
# r19				f19
# r20	0x4330			f20
# r21	sadjust			f21
# r22	bbextents		f22
# r23	tadjust			f23
# r24	bbextentt		f24
# r25	pspan			f25
# r26	sstep			f26
# r27	tstep			f27	<unused>
# r28	count			f28	d_tdivzstepu
# r29	spancount		f29	d_sdivzstepu
# r30	pbase			f30	d_zistepu
# r31	pdest			f31	tmp

D_DrawSpans16:
_D_DrawSpans16:

# This code mostly copied from DrawSpans8()

		stwu	r1,-128(r1)
		mflr	r0
		stfd	f27,88(r1)
		stfd	f28,96(r1)
		stfd	f29,104(r1)
		stfd	f30,112(r1)
		stfd	f31,120(r1)
		stw	r14,16(r1)
		stw	r15,20(r1)
		stw	r16,24(r1)
		stw	r17,28(r1)
		stw	r18,32(r1)
		stw	r19,36(r1)
		stw	r20,40(r1)
		stw	r21,44(r1)
		stw	r22,48(r1)
		stw	r23,52(r1)
		stw	r24,56(r1)
		stw	r25,60(r1)
		stw	r26,64(r1)
		stw	r27,68(r1)
		stw	r28,72(r1)
		stw	r29,76(r1)
		stw	r30,80(r1)
		stw	r31,84(r1)
		stw	r0,132(r1)

		lis	r11,bbextentt@ha
		lwz	r24,bbextentt@l(r11)

		lis	r9,tadjust@ha
		lwz	r23,tadjust@l(r9)

		lis	r11,bbextents@ha
		lwz	r22,bbextents@l(r11)

		lis	r9,sadjust@ha
		lwz	r21,sadjust@l(r9)

		lis	r20,0x4330

		lis	r10,d_zistepu@ha
		lfs	f30,d_zistepu@l(r10)

		lis	r9,d_sdivzstepu@ha
		lfs	f29,d_sdivzstepu@l(r9)

		lis	r11,d_tdivzstepu@ha
		lfs	f28,d_tdivzstepu@l(r11)

		lis	r9,cachewidth@ha
		lwz	r18,cachewidth@l(r9)

		lis	r9,.reciprocals-16@ha
		la	r17,.reciprocals-16@l(r9)

		lis	r7,.LC6a@ha
		lfs	f0,.LC6a@l(r7)

		lis	r9,cacheblock@ha
		lwz	r30,cacheblock@l(r9)

		lis	r8,.LC7@ha
		lfd	f3,.LC7@l(r8)

		lis	r7,d_tdivzorigin@ha
		mtlr	r7

		mr	r25,r3			# pspan

		fmuls	f31,f29,f0		# d_sdivzstepu * 16.0
		li	r26,0			# sstep = 0
		li	r27,0			# tstep = 0

		lis	r19,d_sdivzorigin@ha
		fmuls	f1,f28,f0		# d_tdivzstepu * 16.0
		lis	r14,d_viewbuffer@ha
		fmuls	f2,f30,f0		# d_zistepu * 16.0
		lis	r16,0x4330
		lis	r15,d_ziorigin@ha

# outer loop starts here

.L140a:
		lwz	r29,0(r25)
		lis	r9,d_zistepv@ha
		lwz	r3,4(r25)
		lis	r8,.LC8@ha
		xoris	r0,r29,0x8000
		lfs	f8,d_zistepv@l(r9)
		la	r8,.LC8@l(r8)
		stw	r0,12(r1)
		xoris	r9,r3,0x8000
		lis	r6,d_sdivzstepv@ha
		stw	r16,8(r1)
		lis	r4,d_tdivzstepv@ha
		lfd	r12,8(r1)
		lis	r5,screenwidth@ha
		stw	r9,12(r1)
		stw	r16,8(r1)
		lfd	f13,8(r1)
		fsub	f12,f12,f3
		lfs	f11,d_ziorigin@l(r15)
		fsub	f13,f13,f3
		lfs	f4,0(r8)
		frsp	f12,f12
		lfs	f10,d_sdivzorigin@l(r19)
		lfs	f0,d_sdivzstepv@l(r6)
		frsp	f13,f13
		mflr	r7
		lfs	f6,d_tdivzstepv@l(r4)
		fmadds	f8,f13,f8,f11
		lwz	r9,screenwidth@l(r5)
		fmadds	f0,f13,f0,f10
		lfs	f11,d_tdivzorigin@l(r7)
		mullw	r9,r9,r3
		fmadds	f5,f12,f30,f8		# d_zistepu
		lwz	r0,d_viewbuffer@l(r14)
		fmadds	f7,f12,f29,f0		# d_sdivzstepu
		fmadds	f13,f13,f6,f11
		add	r0,r0,r9
		lwz	r28,8(r25)
		fdivs	f4,f4,f5
		add	r31,r0,r29
		fmuls	f0,f7,f4
		fmadds	f8,f12,f28,f13		# d_tdivzstepu
		fctiwz	f9,f0
		stfd	f9,8(r1)
		lwz	r8,12(r1)
		add	r5,r8,r21		# +sadjust
		cmpw	cr0,r5,r22		# ,bbextents
		ble	.L109a
		mr	r5,r22			# =bbextents
		b	.L110a
.L109a:
		nor	r0,r5,r5
		srawi	r0,r0,31
		and	r5,r5,r0
.L110a:
		fmuls	f0,f8,f4
		fctiwz	f13,f0
		stfd	f13,8(r1)
		lwz	r10,12(r1)
		add	r6,r10,r23		# +tadjust
		cmpw	cr0,r6,r24		# ,bbextentt
		ble	.L112a
		mr	r6,r24
		b	.L113a
.L112a:
		nor	r0,r6,r6
		srawi	r0,r0,31
		and	r6,r6,r0
.L113a:
		cmpwi	cr0,r28,16
		lis	r8,.LC8@ha
		la	r8,.LC8@l(r8)
		lfs	f9,0(r8)

#  if (count > 16)
#    zi += zi16stepu;
#  else {
#    spancountminus1 = (float)(count - 1);
#    zi += d_zistepu * spancountminus1;
#  }
#  z = (float)0x10000 / zi;

		ble-	cr0,.xx1a
		fadds	f5,f5,f2	# zi += zi16stepu
		b	.xx2a
.xx1a:
		addi	r0,r28,-1	# r0 = count - 1
		xoris	r0,r0,0x8000
		stw	r0,12(r1)
		stw	r20,8(r1)	# 0x4330
		lfd	f0,8(r1)
		fsubs	f0,f0,f3	# f0 = (float)(count - 1)
		fmadds	f5,f30,f0,f5	# zi += d_zistepu * spancountminus1
.xx2a:
		fdivs	f4,f9,f5	# z = 0x10000 / zi   (18 cycles)

# CPU-intensive loop starts here
# This loop is executed once for every 16 pixels in a span

# do {

.L139a:

#  if (count >= 16)
#    spancount = 16;
#  else
#    spancount = count;
#  count -= spancount;
#  if (count) {

		li	r29,16		# spancount = 16
		cmpwi	cr0,r28,16
		bge+	.L1a
		mr	r29,r28		# spancount = count
.L1a:
		subf.	r28,r29,r28	# count -= spancount
		beq-	.L120a

#    sdivz += sdivz16stepu;
#    tdivz += tdivz16stepu;

		fadds	f7,f7,f31	# sdivz += sdivz16stepu
		fadds	f8,f8,f1	# tdivz += tdivz16stepu

#    snext = (int)(sdivz * z) + sadjust;
#    tnext = (int)(tdivz * z) + tadjust;
#    if (snext > bbextents)
#      snext = bbextents;
#    else if (snext < 16)
#      snext = 16;
#    if (tnext > bbextentt)
#      tnext = bbextentt;
#    else if (tnext < 16)
#      tnext = 16;

		fmuls	f0,f7,f4	# f0 = sdivz * z
		fmuls	f6,f8,f4	# f6 = tdivz * z
		fctiwz	f13,f0		# f13 = (int)(sdivz * z)
		fctiwz	f10,f6		# f10 = (int)(sdivs * z)
		stfd	f13,8(r1)
		lwz	r9,12(r1)	# r9 = (int)(tdivz * z)
		stfd	f10,8(r1)
		add	r12,r9,r21	# snext = r9 + sadjust
		lwz	r8,12(r1)	# r8 = (int)(sdivz * z)
		cmpwi	cr7,r28,16
		cmpw	cr0,r12,r22	# snext > bbextents?
		add	r4,r8,r23	# tnext = r8 + tadjust
		cmpwi	cr5,r12,16	# snext < 16?

#    if (count > 16)
#      zi += zi16stepu;
#    else {
#      spancountminus1 = (float)(count - 1);
#      zi += d_zistepu * spancountminus1;
#    }
#    z = (float)0x10000 / zi;

		ble-	cr7,.xx3a
		fadds	f5,f5,f2	# zi += zi16stepu
		b	.xx4a
.xx3a:
		addi	r0,r28,-1	# r0 = count - 1
		xoris	r0,r0,0x8000
		stw	r0,12(r1)
		stw	r20,8(r1)	# 0x4330
		lfd	f0,8(r1)
		fsubs	f0,f0,f3	# f0 = (float)(count - 1)
		fmadds	f5,f30,f0,f5	# zi += d_zistepu * spancountminus1
.xx4a:
		cmpw	cr1,r4,r24	# tnext > bbextentt?
		cmpwi	cr6,r4,16	# tnext < 16?
		fdivs	f4,f9,f5	# z = 0x10000 / zi   (18 cycles)

		ble+	cr0,.L121a
		mr	r12,r22		# snext = bbextents
		b	.L122a
.L121a:
		bge+	cr5,.L122a
		li	r12,16		# snext = 16
.L122a:
		ble+	cr1,.L124a
		mr	r4,r24		# tnext = bbextentt
		b	.L125a
.L124a:
		bge+	cr6,.L125a
		li	r4,16		# tnext = 16
.L125a:

#    sstep = (snext - s) >> 4;
#    tstep = (tnext - t) >> 4;

		subf	r0,r5,r12	# r0 = snext - s
		subf	r9,r6,r4	# r9 = tnext - t
		srawi	r26,r0,4	# sstep = (snext - s) >> 4
		srawi	r27,r9,4	# tstep = (tnext - t) >> 4
		b	.L127a

.L120a:

# } else {
#    sdivz += d_sdivzstepu * spancountminus1;
#    tdivz += d_tdivzstepu * spancountminus1;

		fmadds	f7,f29,f0,f7	# sdivz += d_sdivzstepu * spancountminus1;
		fmadds	f8,f28,f0,f8	# tdivz += d_tdivzstepu * spancountminus1;

#    snext = (int)(sdivz * z) + sadjust;
#    if (snext > bbextents)
#      snext = bbextents;
#    else if (snext < 16)
#      snext = 16;
#    tnext = (int)(tdivz * z) + tadjust;
#    if (tnext > bbextentt)
#      tnext = bbextentt;
#    else if (tnext < 16)
#      tnext = 16;

		fmuls	f0,f7,f4
		fmuls	f6,f8,f4
		fctiwz	f13,f0
		fctiwz	f10,f6
		stfd	f13,8(r1)
		lwz	r11,12(r1)
		stfd	f10,8(r1)
		lwz	r9,12(r1)
		add	r12,r11,r21	# +sadjust
		add	r4,r9,r23	# +tadjust
		cmpw	cr0,r12,r22	# ,bbextents
		cmpw	cr1,r4,r24	# ,bbextentt
		ble+	.L128a
		mr	r12,r10
		b	.L129a
.L128a:
		cmpwi	cr0,r12,16
		bge+	.L129a
		li	r12,16
.L129a:
		ble+	cr1,.L131a
		mr	r4,r24		# =bbextentt
		b	.L132a
.L131a:
		cmpwi	cr0,r4,16
		bge+	.L132a
		li	r4,16
.L132a:

#    if (spancount > 1) {
#      sstep = (snext - s) / (spancount - 1);
#      tstep = (tnext - t) / (spancount - 1);
#    }

		cmpwi	cr0,r29,2
		cmpwi	cr1,r29,3	# reciprocal doesn't work for 2
		blt-	.L127a
		subf	r26,r5,r12
		subf	r27,r6,r4
		beq-	.L127a		# no need to divide by 1
		bne+	cr1,.L4a
		srawi	r26,r26,1	# sstep = (snext - s) >> 1
		srawi	r27,r27,1	# tstep = (tnext - t) >> 1
		b	.L127a
.L4a:
		slwi	r9,r29,2
		lwzx	r0,r9,r17	# get reciprocal from table
		mulhw	r26,r0,r26	# faster to multiply by reciprocal
		mulhw	r27,r0,r27
.L127a:

# Following is the inner texture-mapping main loop of D_DrawSpans16()
# where the CPU probably spends most of its time --- except perhaps for
# the earlier fmuls and fdiv near .L139a and .L120a.  Unfortunately I
# think I haven't saved a single cycle here over the code generated by
# GCC, but I made it a lot shorter.  It seems to be limited by the speed
# of memory.  The multiply by cachewidth for every pixel is a bottleneck
# too.

# If I've calculated it right, my loop takes 4 cycles per pixel, plus
# setup time --- not bad considering each mullw takes 4 cycles, each
# lbzx takes 2 and each stb takes 3.

# Since spancount is always between 1 and 16, perhaps it could be
# rewritten as a number of cases where several bytes are written to
# memory at once, with sth and stw, preferably aligned.

#  do {
#    *pdest++ = *(pbase + (s >> 16) + (t >> 16) * cachewidth);
#    s += sstep;
#    t += tstep;
#  } while (--spancount > 0);

		srawi	r11,r6,16	# SCIU1 1     r11 = t >> 16
		srawi	r9,r5,16	# SCIU2 1     r9 = s >> 16
		mullw	r11,r11,r18	# MCIU  2-5   r11 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 2     --spancount
		add	r6,r6,r27	# SCIU2 2     t += tstep
		add	r5,r5,r26	# SCIU1 3     s += sstep
		add	r9,r30,r9	# SCIU2 3     r9 = pbase + (s >> 16)
		srawi	r7,r6,16	# SCIU1 4     r7 = t >> 16
		srawi	r8,r5,16	# SCIU2 4     r8 = s >> 16
		lbzx	r0,r9,r11	# LSU   6-7   r0 = *(pbase + ...)
		beq-	.endloopa	# BPU   6
.loopa:
		mullw	r7,r7,r18	# MCIU  7-10  r7 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 7     --spancount
		add	r6,r6,r27	# SCIU2 7     t += tstep

		stb	r0,0(r31)	# LSU   7-9   *pdest = ...

		add	r5,r5,r26	# SCIU1 8     s += sstep
		add	r8,r30,r8	# SCIU2 8     r8 = pbase + (s >> 16)
		srawi	r11,r6,16	# SCIU1 9     r11 = t >> 16
		srawi	r9,r5,16	# SCIU2 9     r9 = s >> 16

		addi	r31,r31,1	# SCIU1 10    pdest++

		lbzx	r0,r8,r7	# LSU   10-11 r0 = *(pbase + ...)
		beq-	.endloopa	# BPU   10

		mullw	r11,r11,r18	# MCIU  11-14 r11 = (t >> 16) * cachewidth
		addic.	r29,r29,-1	# SCIU1 11    --spancount
		add	r6,r6,r27	# SCIU2 11    t += tstep

		stb	r0,0(r31)	# LSU   11-13 *pdest = ...

		add	r5,r5,r26	# SCIU1 12    s += sstep
		add	r9,r30,r9	# SCIU2 12    r9 = pbase + (s >> 16)
		srawi	r7,r6,16	# SCIU1 13    r7 = t >> 16
		srawi	r8,r5,16	# SCIU2 13    r8 = s >> 16

		addi	r31,r31,1	# SCIU1 14    pdest++

		lbzx	r0,r9,r11	# LSU   14-15 r0 = *(pbase + ...)
		bne+	.loopa		# BPU   14

.endloopa:
		cmpwi	cr7,r28,0	# count > 0?
		stb	r0,0(31)	# *pdest = ...
		mr	r5,r12		# s = snext
		mr	r6,r4		# t = tnext
		addi	r31,r31,1	# pdest++
		bgt+	cr7,.L139a
		lwz	r25,12(r25)	# pspan->pnext
		cmpwi	cr0,r25,0
		bne+	.L140a
		lwz	r0,132(r1)
		mtlr	r0
		lwz	r14,16(r1)
		lwz	r15,20(r1)
		lwz	r16,24(r1)
		lwz	r17,28(r1)
		lwz	r18,32(r1)
		lwz	r19,36(r1)
		lwz	r20,40(r1)
		lwz	r21,44(r1)
		lwz	r22,48(r1)
		lwz	r23,52(r1)
		lwz	r24,56(r1)
		lwz	r25,60(r1)
		lwz	r26,64(r1)
		lwz	r27,68(r1)
		lwz	r28,72(r1)
		lwz	r29,76(r1)
		lwz	r30,80(r1)
		lwz	r31,84(r1)
		lfd	r27,120(r1)
		lfd	r28,112(r1)
		lfd	r29,104(r1)
		lfd	r30,96(r1)
		lfd	r31,88(r1)
		la	r1,128(r1)
		blr
.Lfe4a:
		.size	D_DrawSpans16,.Lfe4-D_DrawSpans16

########################################################################
		.section	".rodata"
		.align 2
.LC9:
		.long 0x4f000000
		.align 3
.LC10:
		.long 0x43300000
		.long 0x80000000
		.align 3
.LC11:
		.long 0x41e00000
		.long 0x0

		.section	".text"
		.align	2
		.globl	D_DrawZSpans
		.type	D_DrawZSpans,@function
		.globl	_D_DrawZSpans
		.type	_D_DrawZSpans,@function

		.extern	d_pzbuffer
		.extern	d_zwidth

D_DrawZSpans:
_D_DrawZSpans:
		stwu	r1,-48(r1)
		stw	r26,24(r1)
		stw	r27,28(r1)
		stw	r28,32(r1)
		stw	r29,36(r1)
		stw	r30,40(r1)
		stw	r31,44(r1)

#	izistep = (int)(d_zistepu * 0x8000 * 0x10000);

		lis	r11,.LC9@ha
		lis	r9,d_zistepu@ha
		la	r11,.LC9@l(r11)
		lfs	f0,d_zistepu@l(r9)
		lis	r26,d_pzbuffer@ha
		lfs	f12,0(r11)
		lis	r9,.LC10@ha
		lis	r27,d_zwidth@ha
		la	r9,.LC10@l(r9)
		lfd	f8,0(r9)
		lis	r31,0x4330
		lis	r28,d_ziorigin@ha
		fmuls	f0,f0,f12
		lis	r9,.LC11@ha
		lis	r29,d_zistepv@ha
		la	r9,.LC11@l(r9)
		lis	r30,d_zistepu@ha
		lfd	f7,0(r9)
		fctiwz	f13,f0
		stfd	f13,16(r1)
		lwz	r5,20(r1)

		lfs	f6,d_ziorigin@l(r28)
		lfs	f10,d_zistepv@l(r29)
		lfs	f9,d_zistepu@l(r30)
		lwz	r27,d_zwidth@l(r27)
		lwz	r26,d_pzbuffer@l(r26)
.L171:

#  pdest = d_pzbuffer + (d_zwidth * pspan->v) + pspan->u;
#  count = pspan->count;
#  du = (float)pspan->u;
#  dv = (float)pspan->v;
#  zi = d_ziorigin + dv*d_zistepv + du*d_zistepu;
#  izi = (int)(zi * 0x8000 * 0x10000);

		lwz	r10,0(r3)	# r10 = pspan->u
		lwz	r8,4(r3)	# r8 = pspan->v
		xoris	r11,r10,0x8000	# r11 = pspan->u ^ 0x8000
		xoris	r9,r8,0x8000	# r9 = pspan->v ^ 0x8000
		mullw	r0,r27,r8	# r0 = d_zwidth * pspan->v
		stw	r11,20(r1)
		stw	r31,16(r1)	# 0x4330
		lfd	f12,16(r1)
		stw	r9,20(r1)
		lfd	f13,16(r1)
		fsub	f12,f12,f8	# f12 = (double)pspan->u
		lwz	r4,8(r3)	# count = pspan->count
		fsub	f13,f13,f8	# f13 = (double)pspan->v
		add	r0,r0,r10	# r0 = d_zwidth * pspan->v + pspan->u
		fmadds	f13,f13,f10,f6	# f13 = pspan->v * d_zistepv + d_ziorigin
		add	r0,r0,r0	# r0 = 2*(d_zwidth * pspan->v + pspan->u)
		fmadds	f12,f12,f9,f13	# zi = pspan->u * d_zistepu + f13
		add	r6,r26,r0	# pdest = d_pzbuffer + r0
		fmuls	f0,f12,f7	# f0 = zi * f7
		andi.	r11,r6,2	# pdest & 2
		fctiwz	f11,f0
		stfd	f11,16(r1)
		lwz	r8,20(r1)	# izi = (int)(zi * f7)

#  if ((long)pdest & 0x02) {
#    *pdest++ = (short)(izi >> 16);
#    izi += izistep;
#    count--;
#  }

		beq-	.L164		# branch if (((long)pdest & 2) == 0)
		srawi	r10,r8,16	# r10 = izi >> 16
		addi	r4,r4,-1	# count--
		add	r8,r8,r5	# izi += izistep
		addi	r6,r6,2		# pdest++
		sthx	r10,r26,r0	# *pdest = r10
.L164:

#  if ((doublecount = count >> 1) > 0) {
#    do {
#      ltemp = izi >> 16;
#      izi += izistep;
#      ltemp |= izi & 0xFFFF0000;
#      izi += izistep;
#      *(int *)pdest = ltemp;
#      pdest += 2;
#    } while (--doublecount > 0);
#  }

		srawi.	r7,r4,1		# doublecount = count >> 1
		ble-	.L165
		add	r10,r8,r5	# SCIU1     izi2 = izi + izistep
		slwi	r11,r5,1	# SCIU2     izistep2 = izistep << 1
.loop2:
		addic.	r7,r7,-1	# SCIU1 1   --doublecount
		srawi	r9,r8,16	# SCIU2 1   ltemp = izi >> 16
		clrrwi	r0,r10,16	# SCIU1 2   r0 = izi2 & 0xffff0000
		or	r9,r9,r0	# SCIU2 2   ltemp |= r0
		add	r8,r8,r11	# SCIU1 3   izi += izistep2
		stw	r9,0(r6)	# LSU   3-5 *(int *)pdest = ltemp
		add	r10,r10,r11	# SCIU2 3   izi2 += izistep2
		addi	r6,r6,4		# SCIU1 4   pdest += 2
		bgt+	.loop2		# BPU   4
.L165:

#  if (count & 1)
#    *pdest = (short)(izi >> 16);

		andi.	r0,r4,1		# (count & 1)
		beq-	.L163
		srawi	r0,r8,16	# r0 = izi >> 16
		sth	r0,0(r6)	# *pdest = r0
.L163:

#} while ((pspan = pspan->pnext) != NULL);

		lwz	r3,12(r3)	# pspan = pspan->next
		cmpwi	cr0,r3,0
		bne+	.L171

		lwz	r26,24(r1)
		lwz	r27,28(r1)
		lwz	r28,32(r1)
		lwz	r29,36(r1)
		lwz	r30,40(r1)
		lwz	r31,44(r1)
		la	r1,48(r1)
		blr
.Lfe5:
		.size	D_DrawZSpans,.Lfe5-D_DrawZSpans
